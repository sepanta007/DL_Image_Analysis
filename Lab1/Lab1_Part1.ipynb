{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7a705d20",
      "metadata": {
        "id": "7a705d20"
      },
      "source": [
        "# Lab1 â€” PyTorch Foundations for Computer Vision\n",
        "\n",
        "**Course**: Deep Learning for Image Analysis\n",
        "\n",
        "**Class**: M2 IASD App  \n",
        "\n",
        "**Professor**: Mehyar MLAWEH\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "By the end of this lab, you should be able to:\n",
        "\n",
        "- Understand how **neurons and layers** are implemented in PyTorch\n",
        "- Manipulate **tensors** and reason about shapes\n",
        "- Use **autograd** to compute gradients\n",
        "- Implement a **training loop** yourself\n",
        "- Connect theory (neurons, loss, backprop) to actual code\n",
        "\n",
        "âš ï¸ This notebook is **intentionally incomplete**.  \n",
        "Whenever you see **`# TODO`**, you are expected to write code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07470cd",
      "metadata": {
        "id": "e07470cd"
      },
      "source": [
        "\n",
        "**Deadline:** ðŸ—“ï¸ **Saturday, February 7th (23:59)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60119f3a",
      "metadata": {
        "id": "60119f3a"
      },
      "source": [
        "## ðŸ¤– A small (honest) note before you start\n",
        "\n",
        "Letâ€™s be real for a second.\n",
        "\n",
        " I know you **can use LLMs (ChatGPT, Copilot, Claude, etc.)** to help you with this lab.  \n",
        "And yes, **I use them too**, so donâ€™t worry ðŸ˜„\n",
        "\n",
        "ðŸ‘‰ **You are allowed to use AI tools.**  \n",
        "But hereâ€™s the deal:\n",
        "\n",
        "- Donâ€™t just **copyâ€“paste** code you donâ€™t understand  \n",
        "- Take time to **read, question, and modify** what the model gives you  \n",
        "- If you can solve a block **by yourself, without AI**, thatâ€™s excellent\n",
        "\n",
        "Remember:\n",
        "\n",
        "> AI can write code for you, but **only you can understand it** â€” and understanding is what matters for exams, projects, and real work.\n",
        "\n",
        "Use these tools **as assistants, not as replacements for thinking**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“š Useful documentation (highly recommended)\n",
        "\n",
        "You will often find answers faster (and more reliably) by checking the official documentation:\n",
        "\n",
        "- **PyTorch main documentation**  \n",
        "  https://pytorch.org/docs/stable/index.html\n",
        "\n",
        "- **PyTorch tensors**  \n",
        "  https://pytorch.org/docs/stable/tensors.html\n",
        "\n",
        "- **Neural network modules (`torch.nn`)**  \n",
        "  https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "- **Loss functions** (`BCEWithLogitsLoss`, CrossEntropy, etc.)  \n",
        "  https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n",
        "- **Optimizers** (`SGD`, `Adam`, â€¦)  \n",
        "  https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "If you learn how to **navigate the documentation**, you are already thinking like a real AI engineer ðŸ‘Œ\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f278eff5",
      "metadata": {
        "id": "f278eff5"
      },
      "source": [
        "## PART I"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de614322",
      "metadata": {
        "id": "de614322"
      },
      "source": [
        "## 0) Colab setup â€” GPU check\n",
        "\n",
        "**Instructions**\n",
        "1. In Colab: `Runtime â†’ Change runtime type to GPU T4`\n",
        "2. Select **GPU**\n",
        "3. Save and restart runtime\n",
        "\n",
        "Then run the cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "72e3ba23",
      "metadata": {
        "id": "72e3ba23",
        "outputId": "0d577cbc-8634-4d21-f32f-58e1193910dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# TODO: set the device correctly (cuda if available, else cpu)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcc7ceb1",
      "metadata": {
        "id": "fcc7ceb1"
      },
      "source": [
        "## 1) Imports and reproducibility\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "d0ce2798",
      "metadata": {
        "id": "d0ce2798",
        "outputId": "78233335-25a5-4bbe-932e-7b29affd7c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x788cbb749170>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# TODO: fix the random seed for reproducibility\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9349f5a5",
      "metadata": {
        "id": "9349f5a5"
      },
      "source": [
        "## 2) PyTorch tensors and shapes\n",
        "\n",
        "Tensors are multi-dimensional arrays that support:\n",
        "- GPU acceleration\n",
        "- automatic differentiation\n",
        "\n",
        "Understanding **shapes** is critical in deep learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "d2998b3f",
      "metadata": {
        "id": "d2998b3f",
        "outputId": "a60d1a56-8217-478f-ca31-11fe39a5a28e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a shape: torch.Size([3])\n",
            "b shape: torch.Size([4, 5])\n"
          ]
        }
      ],
      "source": [
        "# Examples\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.randn(4, 5)\n",
        "\n",
        "print(\"a shape:\", a.shape)\n",
        "print(\"b shape:\", b.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d675977",
      "metadata": {
        "id": "0d675977"
      },
      "source": [
        "### ðŸ” Question (answer inside the markdown)\n",
        "- How many dimensions does tensor `b` have?\n",
        "\n",
        "**Answer:** Tensor `b` has 2 dimensions.\n",
        "\n",
        "- What does each dimension represent conceptually?\n",
        "\n",
        "**Answer:** The first dimension represents the number of rows, and the second dimension represents the number of columns. In other words, `b` is a 2D matrix with shape `[4, 5]`: 4 rows and 5 columns.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea0588f",
      "metadata": {
        "id": "9ea0588f"
      },
      "source": [
        "### âœ…Tensor operations\n",
        "\n",
        "Complete the following:\n",
        "\n",
        "1. Create a tensor `x` of shape `(8, 3)` with random values  \n",
        "2. Compute:\n",
        "   - the **mean of each column**\n",
        "   - the **L2 norm of each row**\n",
        "3. Normalize `x` **row-wise** using the L2 norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "b4629e99",
      "metadata": {
        "id": "b4629e99",
        "outputId": "168da77d-3d1e-4db5-9082-38f70df1b170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 3]) torch.Size([3]) torch.Size([8, 1]) torch.Size([8, 3])\n"
          ]
        }
      ],
      "source": [
        "# TODO: create x\n",
        "x = torch.randn(8, 3)\n",
        "\n",
        "# TODO: column mean\n",
        "col_mean = x.mean(dim=0)\n",
        "\n",
        "# TODO: row-wise L2 norm\n",
        "row_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n",
        "\n",
        "# TODO: normalized tensor\n",
        "x_normalized = x / row_norm\n",
        "\n",
        "print(x.shape, col_mean.shape, row_norm.shape, x_normalized.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0f8928",
      "metadata": {
        "id": "4d0f8928"
      },
      "source": [
        "## 3) Artificial neuron â€” from math to code\n",
        "\n",
        "A neuron computes:\n",
        "\n",
        "$$\n",
        "z = \\sum_i w_i x_i + b\n",
        "$$\n",
        "\n",
        "Then applies an activation function:\n",
        "\n",
        "$$\n",
        "y = g(z)\n",
        "$$\n",
        "\n",
        "This section connects directly to the theory seen in class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "6d271c97",
      "metadata": {
        "id": "6d271c97",
        "outputId": "2a4463bf-f9f7-47bc-8eb1-aff76bcbbcdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.8000)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "x = torch.tensor([1.0, -2.0, 3.0])\n",
        "w = torch.tensor([0.2, 0.4, -0.1])\n",
        "b = torch.tensor(0.1)\n",
        "\n",
        "z = torch.sum(x * w) + b\n",
        "z\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db2d7490",
      "metadata": {
        "id": "db2d7490"
      },
      "source": [
        "### Activation functions\n",
        "\n",
        "1. Implement **ReLU**\n",
        "2. Implement **Sigmoid**\n",
        "3. Apply both to `z` and compare the outputs\n",
        "\n",
        "Which activation preserves negative values?\n",
        "\n",
        "**Answer:** None of them.\n",
        "\n",
        "* **ReLU** removes negatives\n",
        "\n",
        "* **Sigmoid** squashes them into positive numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f307df40",
      "metadata": {
        "id": "f307df40",
        "outputId": "1fa5fe67-d4f1-4436-ca08-4e7efc01742a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.), tensor(0.3100))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# TODO\n",
        "def relu(z):\n",
        "    return torch.maximum(z, torch.zeros_like(z))\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "y_relu = relu(z)\n",
        "y_sigmoid = sigmoid(z)\n",
        "y_relu, y_sigmoid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e764019",
      "metadata": {
        "id": "8e764019"
      },
      "source": [
        "## 4) Autograd and gradients\n",
        "\n",
        "PyTorch uses **automatic differentiation** to compute gradients\n",
        "using the **chain rule** (backpropagation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "50f1aab4",
      "metadata": {
        "id": "50f1aab4",
        "outputId": "e2d5a391-6aa3-45a4-800b-88d073d325ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.890000104904175\n",
            "grad w: tensor([-3.4000, -6.8000,  3.4000])\n",
            "grad b: tensor(-3.4000)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor([1.0, 2.0, -1.0], requires_grad=True)\n",
        "w = torch.tensor([0.5, -0.3, 0.8], requires_grad=True)\n",
        "b = torch.tensor(0.2, requires_grad=True)\n",
        "\n",
        "z = torch.sum(x * w) + b\n",
        "loss = (z - 1.0) ** 2\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print(\"loss:\", loss.item())\n",
        "print(\"grad w:\", w.grad)\n",
        "print(\"grad b:\", b.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2c78a9",
      "metadata": {
        "id": "fe2c78a9"
      },
      "source": [
        "### ðŸ” Conceptual question\n",
        "\n",
        "- If `b.grad > 0`, should `b` increase or decrease after a gradient descent step?\n",
        "Explain **why** in one sentence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5bdee3e",
      "metadata": {
        "id": "b5bdee3e"
      },
      "source": [
        "## 5) Toy classification dataset\n",
        "\n",
        "We create a **linearly separable** dataset.\n",
        "\n",
        "Label rule:\n",
        "- class = 1 if `xâ‚ + xâ‚‚ + xâ‚ƒ > 0`\n",
        "- else class = 0\n",
        "\n",
        "This mimics a very simple classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "15c8bc6a",
      "metadata": {
        "id": "15c8bc6a"
      },
      "outputs": [],
      "source": [
        "# TODO: generate a dataset of size N=500 with 3 features\n",
        "N = 500\n",
        "X = torch.randn(N, 3)\n",
        "y = (X.sum(dim=1, keepdim=True) > 0).float()  # shape (N, 1)\n",
        "\n",
        "# TODO: split into train (80%) and validation (20%)\n",
        "split = int(0.8 * N)\n",
        "\n",
        "X_train, X_val = X[:split], X[split:]\n",
        "y_train, y_val = y[:split], y[split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c16fc2",
      "metadata": {
        "id": "79c16fc2"
      },
      "source": [
        "## 6) Model definition\n",
        "\n",
        "We define a small **MLP** (fully-connected network):\n",
        "\n",
        "`3 â†’ 16 â†’ 8 â†’ 1`\n",
        "\n",
        "Activation: ReLU  \n",
        "Output: raw logits (no sigmoid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "d7b69f8d",
      "metadata": {
        "id": "d7b69f8d"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # TODO: Linear 3 â†’ 16\n",
        "            nn.Linear(3, 16),\n",
        "            # TODO: ReLU\n",
        "            nn.ReLU(),\n",
        "            # TODO: Linear 16 â†’ 8\n",
        "            nn.Linear(16, 8),\n",
        "            # TODO: ReLU\n",
        "            nn.ReLU(),\n",
        "            # TODO: Linear 8 â†’ 1\n",
        "            nn.Linear(8, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# TODO: create model and move it to the GPU\n",
        "model = MLP().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c13b2d2",
      "metadata": {
        "id": "9c13b2d2"
      },
      "source": [
        "###  parameters\n",
        "\n",
        "1. Compute **by hand** the total number of parameters\n",
        "2. Verify your answer using PyTorch\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "* Layer 1 ($3 \\to 16$): $(3 \\times 16) + 16 = 48 + 16 = \\mathbf{64}$\n",
        "* Layer 2 ($16 \\to 8$): $(16 \\times 8) + 8 = 128 + 8 = \\mathbf{136}$\n",
        "* Layer 3 ($8 \\to 1$): $(8 \\times 1) + 1 = 8 + 1 = \\mathbf{9}$\n",
        "\n",
        "Total: $64 + 136 + 9 = \\mathbf{209}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "6168e4a5",
      "metadata": {
        "id": "6168e4a5",
        "outputId": "a38d68ef-e397-4812-99a2-2e286af6ffc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# TODO: count parameters with PyTorch\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "total_params\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f204fb",
      "metadata": {
        "id": "19f204fb"
      },
      "source": [
        "## 7) Training loop\n",
        "\n",
        "You must complete the full training loop:\n",
        "- forward pass\n",
        "- loss computation\n",
        "- backward pass\n",
        "- optimizer step\n",
        "\n",
        "Loss: `BCEWithLogitsLoss`\n",
        "Optimizer: `SGD`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d80ad2c9",
      "metadata": {
        "id": "d80ad2c9",
        "outputId": "bd2468fe-641f-4009-89b7-cecf850ff002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | loss = 0.7006815671920776\n",
            "Epoch 5 | loss = 0.6826688051223755\n",
            "Epoch 10 | loss = 0.6627987623214722\n",
            "Epoch 15 | loss = 0.6406792998313904\n",
            "Epoch 20 | loss = 0.6144785284996033\n",
            "Epoch 25 | loss = 0.5832680463790894\n",
            "Epoch 30 | loss = 0.5466195344924927\n",
            "Epoch 35 | loss = 0.5044291019439697\n",
            "Epoch 40 | loss = 0.45812201499938965\n",
            "Epoch 45 | loss = 0.4103253185749054\n",
            "Epoch 50 | loss = 0.3635922074317932\n",
            "Epoch 55 | loss = 0.32014164328575134\n",
            "Epoch 60 | loss = 0.28147804737091064\n",
            "Epoch 65 | loss = 0.24821636080741882\n",
            "Epoch 70 | loss = 0.22020456194877625\n",
            "Epoch 75 | loss = 0.19692149758338928\n",
            "Epoch 80 | loss = 0.17761117219924927\n",
            "Epoch 85 | loss = 0.16155460476875305\n",
            "Epoch 90 | loss = 0.1481115072965622\n",
            "Epoch 95 | loss = 0.13669680058956146\n"
          ]
        }
      ],
      "source": [
        "# TODO: move data to device\n",
        "X_train_d = X_train.to(device)\n",
        "y_train_d = y_train.to(device)\n",
        "X_val_d = X_val.to(device)\n",
        "y_val_d = y_val.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # TODO: forward\n",
        "    logits = model(X_train_d)\n",
        "\n",
        "    # TODO: loss\n",
        "    loss = criterion(logits, y_train_d)\n",
        "\n",
        "    # TODO: backward\n",
        "    loss.backward()\n",
        "\n",
        "    # TODO: update\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(\"Epoch\", epoch, \"| loss =\", float(loss))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c894744",
      "metadata": {
        "id": "5c894744"
      },
      "source": [
        "## 8) Evaluation\n",
        "\n",
        "1. Apply `sigmoid` to the logits\n",
        "2. Convert probabilities to predictions\n",
        "3. Compute **accuracy** on the validation set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "b10b706c",
      "metadata": {
        "id": "b10b706c",
        "outputId": "de530f65-78de-4136-fcda-af67a4a4b377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# TODO: evaluation\n",
        "with torch.no_grad():\n",
        "    logits = model(X_val_d)\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).float()\n",
        "\n",
        "accuracy = (preds == y_val_d).float().mean()\n",
        "accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9698541c",
      "metadata": {
        "id": "9698541c"
      },
      "source": [
        "## 9) Reflection questions (answer inside the markdown)\n",
        "\n",
        "1. Why do we **not** apply sigmoid inside the model?\n",
        "\n",
        "**Answer:** We use `BCEWithLogitsLoss`, which combines sigmoid + binary cross-entropy in a numerically stable way. Applying sigmoid in the model would be redundant and can lead to instability.\n",
        "\n",
        "2. What would happen if we removed all ReLU activations?\n",
        "\n",
        "**Answer:** Without ReLU, the network becomes fully linear. Multiple linear layers collapse into one, losing the ability to model non-linear patterns.\n",
        "\n",
        "3. How does this toy problem relate to image classification?\n",
        "\n",
        "**Answer:** This is a simplified version of classification: just like predicting class labels from pixels, we predict labels from features. The same principles (forward pass, loss, backprop, evaluation) apply.\n",
        "\n",
        "Write short answers (2â€“3 lines each).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9f2ed3",
      "metadata": {
        "id": "be9f2ed3"
      },
      "source": [
        "## 10) Bridge to Computer Vision\n",
        "\n",
        "So far:\n",
        "- inputs = vectors of size 3\n",
        "- layers = fully-connected\n",
        "\n",
        "Next session:\n",
        "- inputs = images `(B, C, H, W)`\n",
        "- layers = convolutions\n",
        "- same training logic\n",
        "\n",
        "ðŸ‘‰ **Architecture changes, learning principles stay the same.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f479aad6",
      "metadata": {
        "id": "f479aad6"
      },
      "source": [
        "## Part II â€” Training on MNIST\n",
        "\n",
        "Check the next notebook"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}